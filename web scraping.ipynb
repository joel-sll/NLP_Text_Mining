{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 19.9/25.1 MB 104.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 99.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-18.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "try:\n",
    "    import pyarrow\n",
    "except:\n",
    "    %pip install pyarrow\n",
    "    import pyarrow\n",
    "try:\n",
    "    import bs4\n",
    "    from bs4 import BeautifulSoup\n",
    "except:\n",
    "    %pip install beautifulsoup4\n",
    "    import bs4\n",
    "    from bs4 import BeautifulSoup\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    %pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Restaurant(TypedDict):\n",
    "    source_page: str\n",
    "    name: str\n",
    "    claimed: bool\n",
    "    review_number: int\n",
    "    overall_rating: float\n",
    "    ranking: dict[str:int]\n",
    "    geographic_location: str\n",
    "    price_range:str\n",
    "    address: str\n",
    "    phone_number: str\n",
    "    opening_hours: list[str]\n",
    "    detailed_rating: dict[str:int]\n",
    "    details: dict[str: list[str]]\n",
    "\n",
    "class Reviews(TypedDict):\n",
    "    n_contrib: int\n",
    "    review_score: float\n",
    "    review_title: str\n",
    "    review_body: str\n",
    "    visit_date: dict[str:str]\n",
    "    visit_context: str\n",
    "    review_date: dict[str:str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 AppleWebKit/605.1.15 Version/17.4.1 Safari/605.1.15\",\n",
    "#        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,/;q=0.8\",\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }\n",
    "\n",
    "urls = [\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7612326-Reviews-L_Argot-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d949361-Reviews-Le_Casse_Museau-Lyon_Rhone_Auvergne_Rhone_Alpes.html\", \n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d14913909-Reviews-BLO_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d4059959-Reviews-Mama_Restaurant_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d5539701-Reviews-L_Institut_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d1331945-Reviews-La_Gargotte-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d4993538-Reviews-Le_Boeuf_D_argent-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d1395616-Reviews-Restaurant_Le_Musee-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d12337867-Reviews-Aromatic_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restaurant_scraper(urls=[\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d4059959-Reviews-Mama_Restaurant_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"], save_path=\"data\", headers=headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def restaurant_scraper(urls: list[str], save_path: str, headers: dict[str]):\n",
    "    names = [urls[i].split('-')[4] for i in range(len(urls))]\n",
    "    for i in range(len(urls)):\n",
    "        filename = os.path.join(save_path, f\"{names[i]}.html\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        r = requests.get(urls[i], headers= headers)\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "            time.sleep(5)\n",
    "       \n",
    "def parse_reviews(restaurant: dict[any]) -> list[Reviews]:\n",
    "    REVIEWS_PER_PAGE = 15\n",
    "    SLEEP_TIME = 10\n",
    "\n",
    "    url = restaurant[\"source_page\"].split(\"-\")\n",
    "    n_reviews = int(restaurant[\"review_number\"])\n",
    "    n_review_pages =  n_reviews // 15 + 1 if n_reviews % 15 != 0 else 0\n",
    "    reviews = []\n",
    "    \n",
    "    for n_page in range(1,n_review_pages+1):\n",
    "        fail = 0\n",
    "        url.insert(4, f\"or{REVIEWS_PER_PAGE * n_page}\")\n",
    "        r = requests.get(\"-\".join(url), headers=headers)\n",
    "        while r.status_code != 200:\n",
    "            fail +=1\n",
    "            print(\"request failed {fail} times.\")\n",
    "            time.sleep(SLEEP_TIME * fail)\n",
    "            r = requests.get(\"-\".join(url), headers=headers)\n",
    "            assert fail < 10, \"Failed to recover the reviews 10 times. Please try again later.\"\n",
    "        soup_r = BeautifulSoup(r.text).find_all(\"div\", {\"data-automation\": \"reviewCard\"})\n",
    "        print(f\"parsing reviews page {n_page} out of {n_review_pages}.\")\n",
    "        for current_review in soup_r:\n",
    "            review = {\n",
    "                \"n_contrib\": int(current_review.select_one(\"span.b\").text) if current_review.select_one(\"span.b\") is not None else '-999',\n",
    "                \"review_score\" : float(current_review.select_one('title').text.split()[0].replace(\",\", \".\")),\n",
    "                \"review_title\" : current_review.find(\"div\", {\"data-test-target\": \"review-title\"}).text,\n",
    "                \"review_body\" : current_review.select('div[data-test-target=\"review-body\"] > span > div > div')[0].text,\n",
    "                \"visit_date\" : dict(zip([\"month\", \"year\"],current_review.select_one('div[data-test-target=\"review-title\"]').findNextSibling().text.split()[:2])),\n",
    "                \"visit_context\": current_review.select_one('div[data-test-target=\"review-title\"]').findNextSibling().text.split()[-1],\n",
    "                \"review_date\" : dict(zip([\"day\", \"month\", \"year\"],current_review.find_all('div', recursive=False)[-1].text.split()[2:]))\n",
    "            }\n",
    "            reviews.append(review)\n",
    "        time.sleep(SLEEP_TIME)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_URL(path: str) -> bool:\n",
    "    try:\n",
    "        parsed_path = urlparse(path)\n",
    "        return all([parsed_path.scheme, parsed_path.netloc])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_page(url: str) -> str:\n",
    "    headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 AppleWebKit/605.1.15 Version/17.4.1 Safari/605.1.15\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,/;q=0.8\",\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.8\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "        }\n",
    "    fail = 0\n",
    "    while True:\n",
    "        r = requests.get(url, headers= headers)\n",
    "        if r.status_code == 200:\n",
    "            return r.text\n",
    "            break\n",
    "        fail += 1\n",
    "        assert fail < 10, f\"Failed to grab the page 10 times with error {r.status.code}. Please try again later.\"\n",
    "        print(f\"Failed to grab the page {fail} times. Waiting {fail * 5}s before retrying.\")\n",
    "        time.sleep(5 * fail)\n",
    "    return r.text\n",
    "\n",
    "def get_file(file_path: str) -> str:\n",
    "    assert os.path.isfile(file_path), f\"File {file_path} does not exist or is not a file.\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        html = f.read()\n",
    "    return html\n",
    "\n",
    "def page_parser(file_path: str) -> Restaurant:\n",
    "\n",
    "    if is_URL(file_path):\n",
    "        print(\"downloading page\")\n",
    "        html = get_page(file_path)\n",
    "    else:\n",
    "        print(\"recovering file from disk\")\n",
    "        html = get_file(file_path)\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    restaurant_details = soup.find(\"div\", {\"data-test-target\": \"restaurant-detail-info\"})\n",
    "    photos = soup.find(\"div\", {\"data-section-signature\": \"photo_viewer\"})\n",
    "    photos_dict={}\n",
    "    for img in photos.findAll(\"img\", alt=lambda x: x and x.strip()):\n",
    "        photos_dict[img[\"alt\"]] = img[\"src\"]\n",
    "    overview_tabs = soup.find(\"div\", {\"data-automation\": \"OVERVIEW_TAB_ELEMENT\"})\n",
    "    reviews_ratings = soup.select('div[aria-label=\"Filtrer les avis\"] > div > div')\n",
    "    categories = [cat.select_one(\"div.o\").text for cat in reviews_ratings]\n",
    "    numbers = [tag.select_one(\"div:nth-child(3)\").text.replace(u'\\u202f', '') for tag in reviews_ratings]\n",
    "    opening_days = soup.select(\"div.f.e > div.f.e.Q3 > div.f\")\n",
    "    \n",
    "\n",
    "    restaurant = {\n",
    "        \"source_page\" : soup.find(\"link\", {\"rel\": \"canonical\"})[\"href\"],\n",
    "        \"name\" : restaurant_details.find(\"h1\").text,\n",
    "        \"claimed\" : True if restaurant_details.select(\"span > div\")[0].text == 'Page attribuée' else False,\n",
    "        \"review_number\" : int(soup.find(\"a\", {\"href\": \"#REVIEWS\"}).select_one(\"div > span\").text.replace(u\"\\u202f\", \"\").split()[0]),\n",
    "        \"overall_rating\" : float(restaurant_details.find(\"title\").text.split()[0].replace(\",\", \".\")),\n",
    "        \"ranking\" : {\"rank\": int(restaurant_details.find(\"b\").parent.text.split()[1]),\n",
    "                \"over\": int(restaurant_details.find(\"b\").parent.text.replace(u\"\\u202f\", \"\").split()[3])},\n",
    "        \"geographic_location\" : restaurant_details.find(\"b\").parent.text.split()[-1],\n",
    "        \"price_range\" : restaurant_details.find_all(\"div\", recursive=False)[1].find_all(\"span\", recursive=False)[2].text.split(\",\")[0],\n",
    "        \"address\" : restaurant_details.select(\"span > div\")[3].text.strip(),\n",
    "        \"phone_number\" : restaurant_details.find_all(\"div\", recursive=False)[2].find_all(\"span\", recursive=False)[1].text,\n",
    "        \"opening_hours\" : [tag.text for line in opening_days for tag in line],\n",
    "        \"travelers_choice\" : overview_tabs.select(\"div.biGQs._P.pZUbB.KxBGd > div\")[0].text if len(overview_tabs.select(\"div.biGQs._P.pZUbB.KxBGd > div\")) != 0 else '',\n",
    "\n",
    "        \"detailed_rating\" : {cat: int(num) for cat, num in zip(categories, numbers)},\n",
    "\n",
    "        \"details\" : {tag.text: re.split(r'(?=[A-Z])',tag.findNextSibling().text)[1:] for tag in soup.select(\"div > div.Wf\")},\n",
    "        \"photos\": photos_dict\n",
    "    }\n",
    "    reviews = parse_reviews(restaurant=restaurant)\n",
    "    restaurant[\"reviews\"] = reviews\n",
    "    return restaurant\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load page from tripadvisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant = page_parser(\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7612326-Reviews-L_Argot-Lyon_Rhone_Auvergne_Rhone_Alpes.html\")\n",
    "df = pd.DataFrame.from_dict(restaurant, orient='index').transpose()\n",
    "os.makedirs(\"parsed\", exist_ok=True)\n",
    "df.to_parquet(\"./parsed/\"+restaurant[\"name\"]+\".parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load html files from {PATH_TO_HTML} folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\Aromatic_Restaurant.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\BLO_Restaurant.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\La_Gargotte.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\Le_Boeuf_D_argent.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\Le_Casse_Museau.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\L_Argot.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\L_Institut_Restaurant.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\Mama_Restaurant_Lyon.html',\n",
       " 'c:\\\\Users\\\\Joel\\\\Documents\\\\M2 Sise\\\\M2 SISE - NLP Text Mining\\\\Projet\\\\data\\\\Restaurant_Le_Musee.html']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_HTML = \"data\"\n",
    "\n",
    "files = [os.path.join(os.getcwd(),PATH_TO_HTML, file) for file in os.listdir(PATH_TO_HTML) if os.path.isfile(os.path.join(PATH_TO_HTML, file))]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovering file from disk\n"
     ]
    }
   ],
   "source": [
    "restaurant = page_parser(files[0])\n",
    "df = pd.DataFrame.from_dict(restaurant, orient='index').transpose()\n",
    "df.to_parquet(restaurant[\"name\"]+\".parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
